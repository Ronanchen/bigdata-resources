{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: DataFrame API and Spark SQL\n",
    "\n",
    "\n",
    "## Social characteristics of the Marvel Universe\n",
    "\n",
    "In this lab you be working with a dataset that was creted about the Marvel Comics universe. The source data is a text file that was created with a graph analysis library outside of Spark, so the text file contains a lot of information and is not in a format that is easy to query with SQL. You are going to use it in this lab to practice data ingestion, manipulation and analysis using both the DataFrame API and Spark SQL. You can see more about the source data [here](http://bioinfo.uib.es/~joemiro/marvel.html).\n",
    "\n",
    "## Understanding the data file\n",
    "\n",
    "As mentioned above, the data file contains information about Marvel characters, publications, and which character appeared in each publication. There are two sections in the file:\n",
    "\n",
    "- Vertices (section begins in line 1 with a header in the form of `*Vertices 19428 6486`):\n",
    "    - Characters: lines 2-6487 in the format `6481 \"DETHSTRYK\"`, where `6481` is the node id and the name is within double quotes\n",
    "    - Publications: lines 6488-19429 in the same format as characters\n",
    "- Edgeslist (section begins in line 19430 with a header in the form of `*Edgeslist`): lines 19431 to the end of the file. The edge information is in the following format: `2 6488 6489 6490 6491 6492 6493 6494 6495 6496`. This represents a relationship between the character id (the first number) and the publication id's (all other numbers.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all your Spark related environment variables, and pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your SparkSession. You are only going to create a `SparkSession`, not a `SparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"marvel\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your SparkSession is active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-47-245.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>marvel</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4fd010dda0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data.\n",
    "\n",
    "Although the data is a flat file with the structure explained earlier, you will be working with this file using [Spark DataFrame API and Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html).\n",
    "\n",
    "Load in the text file using [Generic load functions for SparkSQL](https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#data-sources), which is located at `s3://bigdatateaching/marvel/porgat.txt`. This file is also located in Azure Blob at `wasbs://marvel@bigdatateaching.blob.core.windows.net/porgat.txt`. \n",
    "\n",
    "Create a DataFrame called `df_in` with a single field, where each record is the full text per line in the original text file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = spark.read.text(\"s3://bigdatateaching/marvel/porgat.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first few lines of `df_in`. What is the default field name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|*Vertices 19428 6486|\n",
      "|1 \"24-HOUR MAN/EM...|\n",
      "|2 \"3-D MAN/CHARLE...|\n",
      "|3 \"4-D MAN/MERCURIO\"|\n",
      "|         4 \"8-BALL/\"|\n",
      "|               5 \"A\"|\n",
      "|           6 \"A'YIN\"|\n",
      "|    7 \"ABBOTT, JACK\"|\n",
      "|         8 \"ABCISSA\"|\n",
      "|            9 \"ABEL\"|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows in `df_in`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30520"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to a DataFrame when you bring it into your Python session from the cluster? Get the first few records of the `df_in` and save it into an object in your Python session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_10_python = df_in.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the local Python object. How is it different than the DataFrame in the cluster? Read up on the [Pyspark Row object](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=row#pyspark.sql.Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='*Vertices 19428 6486'),\n",
       " Row(value='1 \"24-HOUR MAN/EMMANUEL\"'),\n",
       " Row(value='2 \"3-D MAN/CHARLES CHAN\"'),\n",
       " Row(value='3 \"4-D MAN/MERCURIO\"'),\n",
       " Row(value='4 \"8-BALL/\"'),\n",
       " Row(value='5 \"A\"'),\n",
       " Row(value='6 \"A\\'YIN\"'),\n",
       " Row(value='7 \"ABBOTT, JACK\"'),\n",
       " Row(value='8 \"ABCISSA\"'),\n",
       " Row(value='9 \"ABEL\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_10_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_in_10_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange the data\n",
    "\n",
    "Since the data is in text format, at the end of this section you will have two DataFrames: one for nodes (both characters and publications) and one for edges (the relationship between characters and publications.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame API and SparkSQL functions are in the [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) library. You can import all of the functions or specific functions as needed.\n",
    "\n",
    "Start by importing the `monotonically_increasing_id` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe called `df_in_idx` where you will add a new field called `idx` that uses the `monotonically_increasing_id` to add a unique incremental numeric index to each record. You should read more about the function and understand how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_idx = df_in \\\n",
    "    .withColumn(\"idx\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See your new dataframe, you will see the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               value|idx|\n",
      "+--------------------+---+\n",
      "|*Vertices 19428 6486|  0|\n",
      "|1 \"24-HOUR MAN/EM...|  1|\n",
      "|2 \"3-D MAN/CHARLE...|  2|\n",
      "|3 \"4-D MAN/MERCURIO\"|  3|\n",
      "|         4 \"8-BALL/\"|  4|\n",
      "|               5 \"A\"|  5|\n",
      "|           6 \"A'YIN\"|  6|\n",
      "|    7 \"ABBOTT, JACK\"|  7|\n",
      "|         8 \"ABCISSA\"|  8|\n",
      "|            9 \"ABEL\"|  9|\n",
      "+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in_idx.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               value|  idx|\n",
      "+--------------------+-----+\n",
      "|*Vertices 19428 6486|    0|\n",
      "|          *Edgeslist|19429|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in_idx.filter(df_in.value.rlike('^\\*')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from the data file that the two headers are in lines 1 and 19430, and we want those lines (records) from the data. Create a new dataframe called `df_idx_no_hdr` where using a sql query, you select all records but those with the header.\n",
    "\n",
    "Before you can run a SparkSQL query, you need to \"register\" a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the header rows\n",
    "df_in_idx.createOrReplaceTempView(\"df_in_idx\")\n",
    "df_idx_no_hdr = spark.sql('select * from df_in_idx where idx !=0 and idx != 19429')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the header rows were removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30518"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx_no_hdr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will create three separate dataframes: a `characters` one, a `publications` one, and a `relationships` one. You know the original line indices that partition the file, so use those. You have the `idx` field to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "characters = df_idx_no_hdr.filter(col(\"idx\") <= 6486)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6486"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = df_idx_no_hdr \\\n",
    "    .filter((col(\"idx\") >6486) & (col(\"idx\") <= 19428))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12942"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = df_idx_no_hdr \\\n",
    "    .filter(df_idx_no_hdr[\"idx\"] > 19428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11090"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the characters and publications dataframes in order to run SQL commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT regexp_extract('100-200', '(\\d+)-(\\d+)', 1);\n",
    "# 100\n",
    "characters.createOrReplaceTempView(\"char\")\n",
    "publications.createOrReplaceTempView(\"pub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `regexp_extract` function within a SQL statement and a regular expression to create three fields from the whole line: the first field will be the integer (which is the node id), the second is the text **between** the double quotes, and the third wether it is a character or publication. Create two new separate datframes, one for characters and the other for publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df = spark.sql(\"select idx as node_id, regexp_extract(value, '^[0-9]+ \\\"(.*)\\\"$', 1) as name, \\\"character\\\" as node_type from char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "publication_df = spark.sql(\"select idx as node_id, regexp_extract(value, '^[0-9]+ \\\"(.*)\\\"$', 1) as name, \\\"publication\\\" as node_type from pub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+\n",
      "|node_id|                name|node_type|\n",
      "+-------+--------------------+---------+\n",
      "|      1|24-HOUR MAN/EMMANUEL|character|\n",
      "|      2|3-D MAN/CHARLES CHAN|character|\n",
      "|      3|    4-D MAN/MERCURIO|character|\n",
      "|      4|             8-BALL/|character|\n",
      "|      5|                   A|character|\n",
      "|      6|               A'YIN|character|\n",
      "|      7|        ABBOTT, JACK|character|\n",
      "|      8|             ABCISSA|character|\n",
      "|      9|                ABEL|character|\n",
      "|     10|ABOMINATION/EMIL BLO|character|\n",
      "|     11|ABOMINATION | MUTANT|character|\n",
      "|     12|         ABOMINATRIX|character|\n",
      "|     13|             ABRAXAS|character|\n",
      "|     14|          ADAM 3,031|character|\n",
      "|     15|             ABSALOM|character|\n",
      "|     16|ABSORBING MAN/CARL C|character|\n",
      "|     17|ABSORBING MAN | MUTA|character|\n",
      "|     18|                ACBA|character|\n",
      "|     19|ACHEBE, REVEREND DOC|character|\n",
      "|     20|            ACHILLES|character|\n",
      "|     21|  ACHILLES II/HELMUT|character|\n",
      "|     22|  ACROBAT/CARL ZANTE|character|\n",
      "|     23|              ADAM X|character|\n",
      "|     24|        ADAMS, CINDY|character|\n",
      "|     25|ADAMS, CONGRESSMAN H|character|\n",
      "|     26|       ADAMS, GEORGE|character|\n",
      "|     27|       ADAMS, MARTHA|character|\n",
      "|     28| ADAMS, NICOLE NIKKI|character|\n",
      "|     29|      ADAMSON, JASON|character|\n",
      "|     30|    ADAMSON, REBECCA|character|\n",
      "|     31|   ADMIRAL PROTOCOL/|character|\n",
      "|     32|               ADORA|character|\n",
      "|     33|         ADORA CLONE|character|\n",
      "|     34|               ADRIA|character|\n",
      "|     35|                ADVA|character|\n",
      "|     36|   ADVENT/KYLE GROBE|character|\n",
      "|     37|           ADVERSARY|character|\n",
      "|     38|  AEGIS/TREY ROLLINS|character|\n",
      "|     39|            AENTAROS|character|\n",
      "|     40|           AFTERLIFE|character|\n",
      "|     41|           AGAMEMNON|character|\n",
      "|     42|AGAMEMNON II/ANDREI |character|\n",
      "|     43|      AGAMEMNON III/|character|\n",
      "|     44|            AGAMOTTO|character|\n",
      "|     45|        AGARN, CAPT.|character|\n",
      "|     46|        AGED GENGHIS|character|\n",
      "|     47|    AGEE, DR. AUBREY|character|\n",
      "|     48|       AGEE, REBECCA|character|\n",
      "|     49|         AGENT AXIS/|character|\n",
      "|     50|AGENT 18/JACK TRUMAN|character|\n",
      "+-------+--------------------+---------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+\n",
      "|node_id|    name|  node_type|\n",
      "+-------+--------+-----------+\n",
      "|   6487|  AA2 35|publication|\n",
      "|   6488|M/PRM 35|publication|\n",
      "|   6489|M/PRM 36|publication|\n",
      "|   6490|M/PRM 37|publication|\n",
      "|   6491|   WI? 9|publication|\n",
      "|   6492|   AVF 4|publication|\n",
      "|   6493|   AVF 5|publication|\n",
      "|   6494|  H2 251|publication|\n",
      "|   6495|  H2 252|publication|\n",
      "|   6496|   COC 1|publication|\n",
      "|   6497|   T 208|publication|\n",
      "|   6498|   T 214|publication|\n",
      "|   6499|   T 215|publication|\n",
      "|   6500|   T 216|publication|\n",
      "|   6501|   T 440|publication|\n",
      "|   6502|   CM 51|publication|\n",
      "|   6503|    Q 14|publication|\n",
      "|   6504|    Q 16|publication|\n",
      "|   6505|  CA3 36|publication|\n",
      "|   6506| SLEEP 2|publication|\n",
      "|   6507| SLEEP 1|publication|\n",
      "|   6508|SLEEP 19|publication|\n",
      "|   6509|  W2 159|publication|\n",
      "|   6510|  W2 160|publication|\n",
      "|   6511|  W2 161|publication|\n",
      "|   6512|XCAL 110|publication|\n",
      "|   6513|XCAL 109|publication|\n",
      "|   6514|XCAL 107|publication|\n",
      "|   6515|XCAL 108|publication|\n",
      "|   6516| DD/SM 1|publication|\n",
      "|   6517|   W2 52|publication|\n",
      "|   6518|   W2 53|publication|\n",
      "|   6519|XFOR 108|publication|\n",
      "|   6520|XFOR 109|publication|\n",
      "|   6521|   H3 25|publication|\n",
      "|   6522|TTA 90/2|publication|\n",
      "|   6523|TTA 91/2|publication|\n",
      "|   6524|   SS 12|publication|\n",
      "|   6525|   T 178|publication|\n",
      "|   6526|  H2 136|publication|\n",
      "|   6527|  H2 137|publication|\n",
      "|   6528|  H2 159|publication|\n",
      "|   6529|  H2 171|publication|\n",
      "|   6530|  H2 194|publication|\n",
      "|   6531|  H2 195|publication|\n",
      "|   6532|  H2 196|publication|\n",
      "|   6533|  H2 270|publication|\n",
      "|   6534|  H2 287|publication|\n",
      "|   6535|  H2 278|publication|\n",
      "|   6536|  H2 288|publication|\n",
      "+-------+--------+-----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "publication_df.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack both the character and publications into a single dataframe called `nodes_df`, and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = char_df.union(publication_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+\n",
      "|node_id|                name|  node_type|\n",
      "+-------+--------------------+-----------+\n",
      "|  19411|              YC 4/4|publication|\n",
      "|  19412|              YC 4/6|publication|\n",
      "|  19413|            KZ3 11/2|publication|\n",
      "|  19414|            KZ3 12/2|publication|\n",
      "|  19415|               H' 98|publication|\n",
      "|  19416|         SGT. FURY 8|publication|\n",
      "|  19417|   CA: MEDUSA EFFECT|publication|\n",
      "|  19418|SPIDER-MAN: FEAR ITS|publication|\n",
      "|  19419|            TOTZ 4/6|publication|\n",
      "|  19420|              TOTZ 5|publication|\n",
      "|  19421|              TOTZ 6|publication|\n",
      "|  19422|              TOTZ 7|publication|\n",
      "|  19423|              TOTZ 8|publication|\n",
      "|  19424|           BIZADV 33|publication|\n",
      "|  19425|             WI 25/2|publication|\n",
      "|  19426|              AA2 30|publication|\n",
      "|  19427|              AA2 20|publication|\n",
      "|  19428|              AA2 38|publication|\n",
      "+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_df.filter(col(\"node_id\") > 19410).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will work on the relationships dataframe. Import the `split` and `explode` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_rdd = relationships.select(\"value\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = relationships.withColumn(\"newcol\", split(col(\"value\"), \" \")).cache() #convert value to array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using a chain of daframe api methods, start with the value field in the `relationships` dataframe, wher you will create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = relationships.withColumn(\"newcol\", split(col(\"value\"), \" \")) \\\n",
    "    .withColumn(\"character\", col(\"newcol\").getItem(0)) \\\n",
    "    .select(\"character\", explode(col(\"newcol\")).alias(\"publication\")) \\\n",
    "    .filter(col(\"character\") != col(\"publication\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Register relationships as a temporary view\n",
    "nodes_df.createOrReplaceTempView(\"nodes_df\")\n",
    "edges_df.createOrReplaceTempView(\"edges_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=96662)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from edges_df\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an analytical dataset\n",
    "\n",
    "You will now create an analytical dataset using SparkSQL where you will join both tables (nodes_df and edge_df) so you have the data you need to run some analytics on the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|character|publication|\n",
      "+---------+-----------+\n",
      "|        1|       6487|\n",
      "|        2|       6488|\n",
      "|        2|       6489|\n",
      "|        2|       6490|\n",
      "|        2|       6491|\n",
      "|        2|       6492|\n",
      "|        2|       6493|\n",
      "|        2|       6494|\n",
      "|        2|       6495|\n",
      "|        2|       6496|\n",
      "+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+\n",
      "|node_id|                name|node_type|\n",
      "+-------+--------------------+---------+\n",
      "|      1|24-HOUR MAN/EMMANUEL|character|\n",
      "|      2|3-D MAN/CHARLES CHAN|character|\n",
      "|      3|    4-D MAN/MERCURIO|character|\n",
      "|      4|             8-BALL/|character|\n",
      "|      5|                   A|character|\n",
      "|      6|               A'YIN|character|\n",
      "|      7|        ABBOTT, JACK|character|\n",
      "|      8|             ABCISSA|character|\n",
      "|      9|                ABEL|character|\n",
      "|     10|ABOMINATION/EMIL BLO|character|\n",
      "+-------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = spark.sql(\"\"\"select e.character as character_id, c.name as character, \n",
    "                      e.publication as publication_id, p.name as publication\n",
    "                      from edges_df e \n",
    "                      left join nodes_df c on e.character = c.node_id\n",
    "                      left join nodes_df p on e.publication = p.node_id\n",
    "                      \"\"\").cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------+-----------+\n",
      "|character_id|           character|publication_id|publication|\n",
      "+------------+--------------------+--------------+-----------+\n",
      "|           1|24-HOUR MAN/EMMANUEL|          6487|     AA2 35|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6488|   M/PRM 35|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6489|   M/PRM 36|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6490|   M/PRM 37|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6491|      WI? 9|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6492|      AVF 4|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6493|      AVF 5|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6494|     H2 251|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6495|     H2 252|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6496|      COC 1|\n",
      "+------------+--------------------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analytics.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 publications with the highest number of characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|         publication|char_ct|\n",
      "+--------------------+-------+\n",
      "|               COC 1|    111|\n",
      "|MARVEL MYSTERY COMIC|     92|\n",
      "|                IW 3|     91|\n",
      "|                IW 1|     90|\n",
      "|              H2 279|     87|\n",
      "|                IW 4|     80|\n",
      "|                IW 2|     76|\n",
      "|            MAXSEC 3|     72|\n",
      "|              FF 370|     63|\n",
      "|              M/GN 1|     60|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analytics.createOrReplaceTempView(\"analytics\")\n",
    "spark.sql(\"\"\"select publication, count(*) as char_ct from analytics \n",
    "            group by publication order by char_ct desc limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a DataFrame to a csv\n",
    "```\n",
    "df.write\\\n",
    "    .format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"path-in-hdfs-or-cloud\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is based on [https://vincentlauzon.com/2018/01/24/azure-databricks-spark-sql-data-frames/](https://vincentlauzon.com/2018/01/24/azure-databricks-spark-sql-data-frames/). You can see the code in that blog post to perform the same operations using RDD commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parking lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of records in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in.createOrReplaceTempView('df_in_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select row_number() over (order by \"some_column\") as num, * from df_in_view').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_python = spark.sql(\"select count(*) from relationships\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationshipsDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from relationships limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = spark.read.csv(\"s3://bigdatateaching/marvel/relationship\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's do the same for characters\n",
    "charactersDf = spark.createDataFrame(characters.map(lambda t:  Row(charId=t[0], name=t[1])))\n",
    "charactersDf.createOrReplaceTempView(\"characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  and for publications\n",
    "publicationsDf = spark.createDataFrame(publications.map(lambda t:  Row(pubId=t[0], name=t[1])))\n",
    "publicationsDf.createOrReplaceTempView(\"publications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationshipsDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is the standard way of running a SQL query on Spark. This query ranks Marvel characters in duo in order of join-appearances in publications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"\"\"SELECT c1.name AS name1, c2.name AS name2, sub.charId1, sub.charId2, sub.pubCount\n",
    "FROM\n",
    "(\n",
    "  SELECT r1.charId AS charId1, r2.charId AS charId2, COUNT(r1.pubId, r2.pubId) AS pubCount\n",
    "  FROM relationships AS r1\n",
    "  CROSS JOIN relationships AS r2\n",
    "  WHERE r1.charId < r2.charId\n",
    "  AND r1.pubId=r2.pubId\n",
    "  GROUP BY r1.charId, r2.charId\n",
    ") AS sub\n",
    "INNER JOIN characters c1 ON c1.charId=sub.charId1\n",
    "INNER JOIN characters c2 ON c2.charId=sub.charId2\n",
    "ORDER BY sub.pubCount DESC\n",
    "LIMIT 10\"\"\").cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(\"\"\"\n",
    "SELECT c1.name AS name1, c2.name AS name2, c3.name AS name3, sub.charId1, sub.charId2, sub.charId3, sub.pubCount\n",
    "FROM\n",
    "(\n",
    "  SELECT r1.charId AS charId1, r2.charId AS charId2, r3.charId AS charId3, COUNT(r1.pubId, r2.pubId, r3.pubId) AS pubCount\n",
    "  FROM relationships AS r1\n",
    "  CROSS JOIN relationships AS r2\n",
    "  CROSS JOIN relationships AS r3\n",
    "  WHERE r1.charId < r2.charId\n",
    "  AND r2.charId < r3.charId\n",
    "  AND r1.pubId=r2.pubId\n",
    "  AND r2.pubId=r3.pubId\n",
    "  GROUP BY r1.charId, r2.charId, r3.charId\n",
    ") AS sub\n",
    "INNER JOIN characters c1 ON c1.charId=sub.charId1\n",
    "INNER JOIN characters c2 ON c2.charId=sub.charId2\n",
    "INNER JOIN characters c3 ON c3.charId=sub.charId3\n",
    "ORDER BY sub.pubCount DESC\n",
    "LIMIT 10\n",
    "\"\"\").cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
